# 零基础入门深度学习(3) - 神经网络和反向传播算法
## 1 前言 什么是神经网络和反向传播算法？
>单独的感知器或者线性单元这样的单元相互连接在一起形成<mark>神经网络</mark>。即按照一定规则连接起来的多个神经元。  

><mark>反向传播算法（Back Propagation）</mark>：通过了解神经网络每个节点误差项的计算方法和权重更新方法（后面说），可以知道计算一个节点的误差需要先计算和它相连的下一层节点的误差项。所以误差项计算从输出层开始反向推算，顾名思义啦。

## 2 咱们接着展开说说——神经网络 
### 2.1 相关规则
- ①神经元，最左侧为输入层，最右侧为输出层，中间为隐藏层（对外部不可见）  
②只有相邻层之间神经元才会有连接  
③N层和N-1层的所有神经元都连接，叫做全连接神经网络（Full connected）
④每个connection都有一个weight（权值）
⑤卷积神经网络（CNN）、循环神经网络（RNN）都具有不同的连接规则
>连接规则不同，网络类型不同    

![神经元](https://img-blog.csdnimg.cn/4ab9628ac3874625b0b8ee3da825a111.jpeg#pic_center)

### 2.2 输出~
输入向量x到输出向量y的函数：$y = f_n(x)$   
n = network  
想计算输出，就得有输入吧。第一步就是把向量$x$中的每个元素$x_i$赋给输入层的神经元，然后根据公式$y = sigmoid(w^T.x)$来进行改成神经元的值的计算，一直直直直...到都计算完，输出$y_i$连接到一起就成了向量$y$。  
![神经元输出](https://img-blog.csdnimg.cn/cad9348cfb06424a9f5a13f84819cb12.jpeg#pic_center)  
上图即为举例说明怎么求神经元的输出（自行李姐一下~）  

### 2.3 矩阵表示~
#### 2.3.1 隐藏层的矩阵表示
利用上图进行举例  
先排列四个节点计算  
$a_4 = sigmoid(w_41x_1 + w_42x_2 + w_43x_3 + w_4b)$  
$a_5 = sigmoid(w_51x_1 + w_52x_2 + w_53x_3 + w_5b)$   
$a_6 = sigmoid(w_61x_1 + w_62x_2 + w_63x_3 + w_6b)$   
$a_7 = sigmoid(w_71x_1 + w_72x_2 + w_73x_3 + w_7b)$ 
随后定义输入向量$x$和输出向量$y$的权重$w_j$  
$w_4 = [w_41,w_42,w_43,w_44]$  ...  
得到 $a_4 = f(w_4.x)$ ...  
并且写入一个矩阵中，就变成矩阵了（好像白说）  
![矩阵](https://img-blog.csdnimg.cn/b249ccbba28b46d08c385b4bd14a41ba.jpeg#pic_center)  
代入前面一组式子得：$a = f(W.x)$    
- 如下图   

![复杂例子](https://img-blog.csdnimg.cn/1922cb99a2dc4997b0aa14756ebc5e97.jpeg#pic_center)  

![计算结果](https://img-blog.csdnimg.cn/4ae24c63c66640b78f9a7c2e8a61ce89.jpeg#pic_center)

## 3 想知道训练方法吗？——反向传播算法(Back Propagation)  
### 3.1 小概念  
>超参数：一个神经网络的连接方式、网络层数和每层的节点数等参数，并不是学习出来的，而是<mark>人为设置</mark>的参数，这些参数即为~  
### 3.2 算法介绍  δ
通过上面的简述，我相信大家已经简单知道了反向传播算法是怎么个事，下面详细叭叭一会。  
- 误差项的计算：  
    1.输出层节点：$δ_i = y_i(1 - y_i)(t_i - y_i)$   
    δ即为误差项，$y_i$是输出，$t_i$则是i的label  
    2.隐藏层节点：$δ_i = a_i(1 - a_i)∑_k∈outputs * w_kiδ_k$     
    $a_i$是输出，$w_ki$为权重，$δ_k$是下一层节点k的误差项
- 更新权值：$w_ji <- w_ji + ηδ_jx_ji$  
    - $w_ji是i到j权重，η为学习速率，其余下标同理

### 3.3 推导？？yep！ but...
- 先确定神经网络的目标函数，然后用随机梯度下降优化算法去求目标函数最小值时的参数值。（This is 通用套路）（书上说的）  

- 取网络所有输出层节点的误差平方和作为目标函数：$E_d ≡ 1/2 * ∑_i∈outputs * (t_i - y_i)^2$  $E_d$是样本d的误差  
- 随机梯度下降算法对目标函数进行优化：  $w_ji <- w_ji - η∂E_d/∂w_ji$  
- 求出误差 对于每个权重 的偏导数（也就是梯度）,设$net_j$是节点j的加权输入,即$net_j = w_j.x_i$  
- $E_d$ is $net_j$ 's function. And $net_j$ is $w_ji$ 's function.  
根据<mark>链式求导法则</mark>得到：  
$∂d/∂w_ji = ∂E_d/∂net_j * x_ji$

#### 3.3.1 输出层权值训练   
>$w_ji <- w_ji - ηδ_jx_ji$

#### 3.3.2 隐藏层权值训练  
>$δ_j = a_i(1 - a_i)∑_k∈Downstream(j)δ_kw_kj$  

代入3.3中式子即可
ps:这都是啥啊

![问号](https://img-blog.csdnimg.cn/462d33e7959c4e18bc292a4406ed0663.jpeg#pic_center)

## 4 参考链接  
https://www.zybuluo.com/hanbingtao/note/476663