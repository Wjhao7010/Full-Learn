# 零基础入门深度学习(2) - 线性单元和梯度下降
## 1 前言 今天学的线性单元和梯度下降是什么？
>线性单元，简单来说就是另一种感知器。他是在面对线性不可分的数据时，用可导线性函数来作为激活函数的感知器。所以被叫做线性单元。  
>>不单单是学习了线性单元，同时也是通过对它的学习来了解了一些machine learning的概念，我就能掌握<mark>机器学习的基本套路：模型，目标函数，优化算法等。</mark>   

>而梯度下降算法就是一种优化算法。  
举个栗子：计算机不会解方程，所以它在面对求函数极值的时候，它就不会找导数等于0的点（不会找极小值点），但是它会一个一个试！因为算力比人脑强太多。  
梯度是一个向量，它会指向函数上升最快的方向，沿着相反的方向可以经过数次迭代之后（不断修改x值）找到函数最小值点了。通过不断迭代选择x的调整步长，就能进行优化，所以称之为优化算法。

![夸我](https://img-blog.csdnimg.cn/7992b00fab7c4c10b0037f94bad71f7b.jpeg#pic_center)
## 2 下面咱们详细展开说说
### 2.1 关于线性单元那点事  

- 这是一个线性单元的简单构成
![线性单元](https://img-blog.csdnimg.cn/9bf6874618094c258d3511bbbcc52cad.jpeg#pic_center)
    - 它和普通感知器的区别就是激活函数被替换成了解决回归问题的函数（ez的很）
    - 那它的模型呢：其实就是探讨输入x预测输出y，例子：x可以是你的工作年限，也可以是你的技术水平等级，我可以用某种算法（假设的，不靠谱）来预测你的收入：$y=h（x）=w*x+b$  
    实际上我们会加入很多的参数，用一个特征向量去描述。给它整成向量的形式：$y=h(x)=w^T$
    - ps:输出是线性组合的就是线性模型啦

- 训练模型用啥方法？监督学习or无监督学习
    - 监督学习：有x有y。其实就是用包括了输入特征x和输出y（标记，label）的数据来对模型进行训练，让模型能知道这样的一个具体的x应该对应什么样的输出y，学习方法+不断练习=训练成功！
    - 无监督学习：这不就巧了吗。<mark>实际上大部分样本数据只有x没有y，只知道输入没有输出。</mark>   
    eg：语音识别（STT）任务。一大堆语音，但是标注文字很费力。想省力气吗？你可以先聚类，就是按照一定的规律把相似的归为一类，然后用你花费一丢丢力气训练出来的样本，来告诉各种分类如何去完成对应的训练。

- 它的目标函数只考虑监督学习。目标，让误差越来越小~咳咳，附上公式：$e=1/2(y-y')^2$
通过e的相加，得到的模型误差E，目标函数就是为了干掉更多的E，让它越来越小。

### 2.2 关于梯度下降算法的那点事
![梯度下降](C:/Users\26968\Desktop\零基础学习深度学习\2.Linear_unit\image\梯度下降.jpg)
老规矩，开局先上图  
继续，上公式（推导在下面）：  
先这样在那样最后这样
>①![公式1](https://img-blog.csdnimg.cn/3a41ea7f690040c2a7aff07f354622c1.jpeg#pic_center) 
②![公式2](https://img-blog.csdnimg.cn/10ad996282dc4286bdebc171d06b69ee.jpeg#pic_center)    
③将∇E(w)代入  ![公式3](https://img-blog.csdnimg.cn/e1d94b0a4f5d4113b1f4b699bc98aa32.jpeg#pic_center)   
④![公式4](https://img-blog.csdnimg.cn/976e878d05bc4ce1b16a33cc1e29f24c.jpeg#pic_center)  
自然，之前介绍过学习速率η，w是E（模型误差）的参数。里面有个陌生人，∇E(w)，下面开始推导。可以略过，也可以看看，很ez。  
- ∇E(w)的推导（我直接上图）  
![推导1](https://img-blog.csdnimg.cn/931d74ee6a644ef29bee911a8f9d8685.jpeg#pic_center)   
![推导2](https://img-blog.csdnimg.cn/a862f6f754874605be0c97e90fb1afe1.jpeg#pic_center)   
![推导3](https://img-blog.csdnimg.cn/67467bf3e54f47acaa5d0163944aad44.jpeg#pic_center)   
<mark>恭喜你，看完了！我们都有更好的明天！</mark>  

### 2.3 随机梯度下降算法(Stochastic Gradient Descent, SGD)
- 批梯度下降(Batch Gradient Descent)  
![公式3](https://img-blog.csdnimg.cn/e1d94b0a4f5d4113b1f4b699bc98aa32.jpeg#pic_center)    
这个公式更新w需要对所有样本进行计算，所以不利于巨大样本。
- 相比较来说SGD更加实用。因为SGD更新w的迭代只计算 个样本。几个？1个哈哈。这样效率直接飞速提升。
- 上帝为你打开一扇门，就给你关了一扇窗。
![SGDBGD](https://img-blog.csdnimg.cn/7bb26a9e5a274fba99cb19f67adb059e.jpeg#pic_center)   
红色为BGD，紫色为SGD。果然效率高了但是很躁动不安，不过还好结果是好的，向最低点逼近了。
不过这也不完全算是坏事，这有助于我们逃避糟糕数值，获得more perfect的模型。

## 参考资料 
https://www.zybuluo.com/hanbingtao/note/448086