# 零基础入门深度学习(4) - 卷积神经网络 
@[toc]
## 1 前言 什么是卷积神经网络？
> 笼统来说，就是若干卷积层 + Pooling层 + 全连接层组成的神经网络。所用架构模式为INPUT -> [[CONV]*N -> POOL?]*M -> [FC]*K。  

如下图所示  

![卷积神经网络](https://img-blog.csdnimg.cn/ce85fe4e09bb4b8396bad74a02c6d58f.jpeg#pic_center)

### 1.1 Relu激活函数
在最近新的卷积神经网络中，激活函数选择了<</mark>Relu函数</mark>，定义为：$f(x) = max(0,x)$。  
![Relu](https://img-blog.csdnimg.cn/7af74f01884449b1bc1318a086f8ccd0.jpeg#pic_center)  
具有以下优点： 
- 速度快，并且计算代价相对来说很小，比如sigmoid函数需要计算指数和梯度，但是Relu只需要一个max。
- 减轻了梯度消失的问题
- 可以训练更深的网络
- 具有稀疏性：激活率能够刚好满足理想需求  
### 1.2 卷积和全连接有什么不同呢？  
#### 1.2.1 全连接网络  
- 参数数量过大：层数很多，输入层参数很多
- 没有利用像素之间的相对位置信息：有的权重训练起来没什么必要
- 网络层数限制：梯度下降方法很难训练很深的网络  
#### 1.2.2 卷积神经网络  
- 局部连接：不是全连接，因为浪费
- 权值共享：一组连接可以共享一个权重
- 下采样：通过Pooling来减少样本数->减少参数量->提升鲁棒性  
## 2 今天的主角——卷积神经网络（CNN）的展开  
![卷积神经网络](https://img-blog.csdnimg.cn/ce85fe4e09bb4b8396bad74a02c6d58f.jpeg#pic_center)  
### 2.1 具有三维的层结构  
卷积就会得到Feature Map，在上图中的3代表了三套参数，每个Filter可以输入图像进行卷积得到F M，有多少个Filter就能得到多少个Feature Map。  
下采样会得到更小的Feature Map，由Pooling层完成。  
全连接层中的神经元和上一层的Feature中每个神经元相连，并且输出层的神经元也都全相连于上一个全连接层，这样就得到了输出。  
### 2.2 CNN输出值的计算  

#### 2.2.1 卷积层输出的计算
老样子，先举例说明。  
例：5*5的图像，3*3的filter，卷积后会得到什么样的feature map呢？  
![卷积层计算](https://img-blog.csdnimg.cn/6389bfecae4641219475eecb8ad19438.jpeg#pic_center)  

我们需要用这样的公式进行卷积后值的计算。    

![卷积计算公式](https://img-blog.csdnimg.cn/fe0292bd30b74cc9a9cd272092284385.jpeg#pic_center)  
这里面参数很多，所以要简单介绍一下参数的含义：  
$x_i,j$：图像的i行j列的元素  
$w_m,n$：第m行n列的filter权重  
$w_b$：filter的偏置项  
$a_i,j$：feature map的第i行j列元素  
f：激活函数（relu）  
下面是计算举例：  

![计算结果举例](https://img-blog.csdnimg.cn/7c13c6ecad8640d38d22c59dee9b36d5.jpeg#pic_center)  
![计算公式举例](https://img-blog.csdnimg.cn/e5cd8e20931749a68d51810d2c88a04f.jpeg#pic_center)
举例之后，下面可以看到整个feature map的计算过程：  
![计算卷积过程](https://img-blog.csdnimg.cn/7eaa827e0b2143e9b1ede392d9b62d3c.gif#pic_center)  

需要说明的是，在上述卷积计算中，步幅(stride)均为1。当然也可以设置大于1的数，当步幅为2的时候，feature map计算就如下图所示：  
![步幅2-1](https://img-blog.csdnimg.cn/07f0155e76ce42078381ac7055fb8e16.jpeg#pic_center)  
![步幅2-2](https://img-blog.csdnimg.cn/910750e45d9e4c44b94e070a78c64cba.jpeg#pic_center)  
步幅变动之后，feature map大小也会随之发生变化，这说明<mark>图像的大小、步幅和卷积后的feature map大小满足一定的关系</mark>： 

![关系公式](https://img-blog.csdnimg.cn/42f4682ef2c543c0aaded4228b96b52a.jpeg#pic_center)
解释一下参数：  
$W_2$：feature map 卷积前的值  
$W_1$：卷积前图像的宽度  
$F$：filter的宽度  
$P$：Zero Padding的数量（补几圈0）  
$S$：步幅
$H_2$：卷积后的feature map高度  
$H_1$：卷积前的图像的宽度    
下面是深度大于1的卷积计算公式，和式1大同小异：  

![卷积计算公式（深度大于1）](https://img-blog.csdnimg.cn/f269e16cc5234b46af1814e58506cd55.jpeg#pic_center) 
d代表了层数，D为深度  
有多少个filter卷积后就有多少个feature map  
下面即为两个filter计算卷积的过程：  
![两个filter卷积](https://img-blog.csdnimg.cn/f4eba85367614410859a0ad9f59fc8ef.gif#pic_center)  
PS:上述即为体现出了局部连接和权值共享。  
参数数量大大减少。 

#### 2.2.2 Pooling层输出的计算
> Pooling层主要是用来进行下采样操作。去掉feature map中不重要的样本来进一步减少参数数量。  

最常用的Pooling方法为Max Pooling，顾名思义，在样本中取最大值作为样本值。  
下图即为2*2的Max Pooling：  

![2x2maxpooling](https://img-blog.csdnimg.cn/677c949bd1924e2bac2a48a591b59829.png#pic_center)
此外，Meaning Pooling则是取各个样本的平均值作为样本值。  

#### 2.2.3 全连接层 
详情见上一节  

## 3 卷积神经网络的训练
万变不离其宗，训练原理和全连接神经网络相同：<mark>利用链式法则求导计算出损失函数对每个权重的偏导数（即为梯度），根据梯度下降公式更新权重，应用反向传播训练算法。</mark>   
卷积神经网络还涉及到了<mark>局部连接、下采样</mark>等操作，会影响到<mark>误差项</mark>的计算，权值共享会影响权重$w$的梯度（偏导数）计算方法。  
### 3.1 卷积层的训练  
#### 3.1.1 卷积层误差项的训练  
- 步长为1，输入深度为1，filter为1个：  
输入3*3，filter为2*2，stride=1，得到2*2的feature map。    
![2x2feature map](https://img-blog.csdnimg.cn/299c966c9642447d982911e08f31da24.jpeg#pic_center)
上图中参数的关系可以理解成下面公式：  
![加权输入关系](https://img-blog.csdnimg.cn/cba90d3f110f48c48198d19c519d9f95.jpeg#pic_center)
具体参数都解释过了，不再赘述。权重为filter的，a为神经元的，net表示加权，f表示激活函数，conv为卷积，net、W、a都是数组。 so easy~
误差项这么求：  
![误差计算](https://img-blog.csdnimg.cn/a36570ef337f4a99997c81116cf2462b.jpeg#pic_center)  
而后通过blabla的先举例再推导：  
![最终误差计算过程](https://img-blog.csdnimg.cn/d27fb1be2e8a4217867d782b6997f495.jpeg#pic_center)
结果就出来啦！  
还可以写成卷积形式：  
![误差计算卷积形式](https://img-blog.csdnimg.cn/bb2dba9d48674e43adaa2ef2c599549d.jpeg#pic_center)  

- 卷积步长为S时的误差传递：  
步长为S与步长为1的区别： 
![步长差距](https://img-blog.csdnimg.cn/15e75ceb7a264c33b839e719990a4d77.jpeg#pic_center)  
简答总结就是：步长为2的卷积得到的feature map跳过了步长为1时的相应部分，反向计算误差时候，可以对步长为S的sensitivity map进行相应位置补0。

- 输入层深度为D时的误差传递：  
输入深度为D时，filter的深度也必须为D。  
反向计算时：利用filter的第$d_i$通道权重对第$l$层的sensitivity map进行卷积，得到$l-1$层$d_i$通道的sensitivity map：  
![sensitivitymap](https://img-blog.csdnimg.cn/ed1367ec07de41d6932dd2cc59a85d15.jpeg#pic_center)  

- filter数量为N时的误差传递：  
filter数量为N时，输出层的深度也为N。
反向计算误差项时，需要使用全导数公式。因为$l-1$层加权输入影响到了$l$层所有的feature map。  
最后在各组之间将N个偏sensitivity map 按元素相加，得到最终的N$l-1%个层的sensitivity map。
![filter为N](https://img-blog.csdnimg.cn/352ea7970bd24f52a0b3feb76129691e.jpeg#pic_center)  

### 3.1.2 卷积层filter权重梯度的计算  
在得到第层sensitivity map的情况下，计算filter的权重的梯度。卷积层是权重共享的，因此梯度的计算稍有不同。  
![filter权重梯度](https://img-blog.csdnimg.cn/9ffc656323b04c0e8f9988bdb72818b5.jpeg#pic_center)  
![filter权重梯度计算过程](https://img-blog.csdnimg.cn/9f6ed7b7d5ab40c699ca69151e8e6feb.jpeg#pic_center)  
通过计算之后，就能得出相应结果了。

### 3.2 Pooling层的训练
Pooling层没有需要学习的参数，所以它仅仅是把误差传递给上一层即可，不用计算梯度。  

#### 3.2.1 Max Pooling误差项的传递  
假设第$l-1$层大小为4*4，pooling filter大小为2*2，步长为2，这样，max pooling之后，第$l$层大小为2*2。  
![maxpooling](https://img-blog.csdnimg.cn/9c9f597b9fcc4cd59166b85d5e1cf51a.jpeg#pic_center)  
经过blabla的运算之后，得到规律：<mark>对于max pooling，下一层的误差项的值会原封不动的传递到上一层对应区块中的最大值所对应的神经元，而其他神经元的误差项的值都是0。</mark>  

![maxpooling计算结果](https://img-blog.csdnimg.cn/0c162641d2da4aad8d290759fa4e8daf.jpeg#pic_center)

#### 3.2.2 Mean Pooling误差项的传递  
还是blabla运算推导（此处省略1w字），得到规律：<mark>对于mean pooling，下一层的误差项的值会平均分配到上一层对应区块中的所有神经元。</mark>  

![meanpooling计算结果](https://img-blog.csdnimg.cn/d8caa6e7f09642c69e2db19863ebdd15.jpeg#pic_center)  

他还有个高端大气上档次的形式：即克罗内克积(Kronecker product)形式。  

![克罗内克积(Kronecker product)](https://img-blog.csdnimg.cn/6ba0b342811248abb2df227d1042c718.jpeg#pic_center)

至此，介绍完毕啦！